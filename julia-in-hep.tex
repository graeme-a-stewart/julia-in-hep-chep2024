% This paper is Copyright © Graeme Andrew Stewart, Sam Skipsey, 
% authors, 2025.
% Licensed under Creative Commons Attribution 4.0 International (CC BY 4.0), see LICENSE

\documentclass{webofc}
\usepackage{graphicx} % Required for inserting images
\graphicspath{{figures/}}
\usepackage{subcaption}
\usepackage[varg]{txfonts}
\usepackage[utf8]{inputenc}

% Workaround for arXiv, start with "finalizecache" then switch to "frozencache"
\usepackage[finalizecache,cachedir=.]{minted}
% \usepackage[frozencache,cachedir=.]{minted}

\usepackage{natbib}
\usepackage{hyperref}
\usepackage{enumitem} % This seems to cause nested itemize lists to break...?

% For draftversion
\usepackage{lineno}
\linenumbers

\title{Julia in HEP}

\author{\firstname{Graeme Andrew} \lastname{Stewart}\inst{1}\fnsep\thanks{\email{graeme.andrew.stewart@cern.ch}} \and
\firstname{Alexander} \lastname{Moreno Briceño}\inst{2} \and
\firstname{Philippe} \lastname{Gras}\inst{3} \and
\firstname{Benedikt} \lastname{Hegner}\inst{1} \and
\firstname{Uwe} \lastname{Hernandez Acosta}\inst{4,5} \and
\firstname{Tamas} \lastname{Gal}\inst{6} \and
\firstname{Jerry} \lastname{Ling}\inst{7} \and
\firstname{Pere} \lastname{Mato}\inst{1} \and
\firstname{Mikhail} \lastname{Mikhasenko}\inst{8} \and
\firstname{Oliver} \lastname{Schulz}\inst{9} \and
\firstname{Sam} \lastname{Skipsey}\inst{10}
% etc.
}

\institute{CERN, Esplanade des Particules 1, Geneva, Switzerland
\and
Universidad Antonio Nariño, Ibagué, Colombia
\and
IRFU, CEA, Université Paris-Saclay, Gif-sur-Yvette, France
\and
Center for Advanced Systems Understanding, Görlitz, Germany
\and
Helmholtz-Zentrum Dresden-Rossendorf, Dresden, Germany
\and
Erlangen Centre for Astroparticle Physics, Friedrich-Alexander-Universität, Erlangen-Nürnberg, Germany
\and
Laboratory for Particle Physics and Cosmology, Harvard University, Cambridge, MA, USA
\and
Ruhr Universität Bochum, Bochum, Germany
\and
Max-Planck-Institut für Physik, Munich, Germany
\and
School of Physics \& Astronomy, University of Glasgow, Glasgow, United Kingdom, G12 8QQ
}


\abstract{%
Julia is a mature general-purpose programming language, with a large ecosystem
of libraries and more than 12000 third-party packages, which specifically
targets scientific computing. As a language, Julia is as dynamic, interactive,
and accessible as Python with NumPy, but achieves run-time performance on par
with C/C++. In this paper, we describe the state of adoption of Julia in HEP,
where momentum has been gathering over a number of years.

HEP-oriented Julia packages can, via \texttt{UnROOT.jl}, already read HEP's
major file formats, including TTree and RNTuple formats. Interfaces to some of
HEP's major software packages, such as through \texttt{Geant4.jl}, are available
too. Jet reconstruction algorithms in Julia show excellent performance. A number
of full HEP analyses have been performed in Julia.

We show how, as the support for HEP has matured, developments have benefited
from Julia's core design choices, which makes reuse from and integration with
other packages easy. In particular, libraries developed outside HEP for
plotting, statistics, fitting, and scientific machine learning are extremely
useful.

We believe that the powerful combination of flexibility and speed, the wide
selection of scientific programming tools, and support for all modern
programming paradigms and tools, make Julia the ideal choice for a future
language in HEP.}

\begin{document}

\maketitle

\section{Programming Languages in High-Energy Physics}
\label{sec:introduction}

\subsection{HEP Needs}

High-energy physics (HEP) is a large field, consisting of tens of thousands of
researchers, almost all of whom will need to interact with software and
contribute to software projects during their careers~\cite{2024EPJWC.29505023M}.
It is also one of the biggest, if not the biggest, generators of scientific
datasets today, with exabytes of storage used by the LHC
experiments~\cite{Collaboration:2904204}. This data is processed by a huge
corpus of software, estimated to be many tens of millions of lines in
C++~\cite{hsfcwp}.

This brings a challenge for HEP software. From the point of view of \emph{code
efficiency} we require fast and efficient execution, high throughput, and
scalability at large computer centres and across distributed infrastructures.
Considering \emph{human efficiency} we would like a low barrier to entry for
newcomers, the ability to prototype code rapidly, a broad ecosystem of well
maintained packages, and excellent tooling for developers. These features are
needed to make software able to deal efficiently with huge datasets, as well as
accessible to a large group of developers.

\subsection{From Fortran to the C++/Python Era}

In response to changing technology, and the needs of the field, the programming
languages that are dominant in the field have evolved over time.
From~\cite{pivarski2022} we can identify three major shifts as seen in
\ref{fig:hep-languages}. 

\begin{figure}[htbp]
    \begin{center}
        \includegraphics[width=0.7\textwidth]{hep-programming-languages.png}
        \caption{Illustrative cartoon of major shifts in HEP programming languages, from~\cite{pivarski2022}.}
        \label{fig:hep-languages}
    \end{center}
\end{figure}

\begin{description}
    \item[From assembler to Fortran] As computers developed from early
    specialised behemoths, with barely an operating system, improved programming
    languages became available. For technical computing Fortran was the most
    effective language and HEP quickly adopted it as it brought a much improved
    syntax, as well as hardware portability.
    \item[From Fortan to C++] Although Fortran had many advantages, HEP had to
    develop language extensions to introduce concepts such as more advanced data
    structures~\cite{Zoll:2296399}. A language which offered native object
    orientation was an attractive choice. In addition, the gap afforded by the
    end of the LEP accelerator and the construction of the LHC allowed the field
    to afford the time for a major language shift.
    \item[The rise of Python] Python earned a well deserved reputation as an
    excellent language for programming efficiency, and also gained ground
    through being the de facto interface to many machine learning libraries. It
    has become widely used in HEP as a compliment to C++.
\end{description}

The current situation for HEP is that C++ and Python are now both widely used,
with each bringing specific advantages, as well as drawbacks. Good C++ excels at
runtime efficiency, but is a difficult language to learn and master as well as
suffering from memory safety issues and being difficult to compose. Python is
expressive, much easier to work with, is safer with memory and composes better
(duck typing). However, it is very slow compared to C++, so not suitable for
high throughput computing.

As noted in~\cite{eschle2023potential}, using two languages is not ideal: it
increases the required expertise, necessitates reimplementation of code for
performance, and reduces code reusability.

\section{Julia}

\subsection{Julia's Motivations}

The Julia programming language was announced in 2012 with a blog post entitled
``Why We Created Julia''~\cite{why-create-julia}. This document lists a series of
ambitious goals for the language, and represents a view into the core
developers' mindset formalised in a later
paper~\cite{bib:julia_freshapproach,10.1145/3276490}.

Julia provides a syntax as productive as Python, especially for numerical work,
whilst leveraging JAOT\footnote{Just-Ahead-Of-Time} compilation to provide speed
similar to compiled, statically typed languages like C and Rust. It utilises
type inference to allow coding in a ``gradually typed'', generic programming
style, although the language will always track types for performance behind the
scenes, and types can be specified explicitly if required. Like MatLab and
Fortran, Julia's native operations and type system support arrays as first-class
entities, of any dimension, allowing array-oriented code to be both productively
generated, and efficiently executed, with operations naturally ``broadcasted''
element- or dimension-wise. This allows Julia to also be effective for writing
linear algebra heavy code. In addition, influence from the R community provides
a wealth of statistical packages, and an R-flavoured approach to plots and
visualisation. Of course, as with most languages of the 21st century, Julia is a
fully open-source language, with its entire codebase freely available (and
almost all of it written in Julia itself).

\subsection{Julia in Practice}
\label{sec:juliainpractice}

Julia's syntax is familiar to programmers conversant with programming performant
code in Python with NumPy, except that whitespace is not syntactically relevant
(blocks end with \texttt{end}).

For example, the listing in Figure~\ref{code:madelbrot} shows some toy code to
generate a grayscale image of the famous Mandelbröt set.

\begin{figure}[!ht]
\centering
\begin{minted}[linenos]{julia}
using Images

function mandel(z)
    c = z
    maxiter = 80
    for n = 1:maxiter
        if abs2(z) > 4
            return n-1
        end
        z = z^2 + c
    end
    maxiter
end

set = [ mandel(complex(r,i)) for i=-1.:.01:1., r=-2.0:.01:0.5 ]
img = Gray.(set ./ 80)
\end{minted}
\caption{Julia implementation of the Mandelbr\"{o}t set}
\label{code:madelbrot}
\end{figure}

Here we demonstrate importing of packages (line 1), function declaration with
implicit types (line 3) and explicit loop (line 6) and branch constructs (line
7), implicit loops via ``comprehensions'' (line 15) to generate a  value for
every point in an implicitly defined array, and, finally, transparent
broadcasting of operations over that array (line 16 -- both the function call to
Gray, and the division operation are distributed over the whole array). Unlike
Python, explicit loops are optimised efficiently, and are no slower than
comprehensions or ``functional-style'' iterators (which are also supported by
Julia).

%Tooling and ecosystem.
As with other languages of the 2010s, Julia comes with a set of tools for
managing development environments beyond the interpreter.

\texttt{juliaup} allows seamless management of multiple Julia releases on the same
machine, including tracking particular patch releases, and choosing the system
default.

The integrated package management in Julia -- via the Pkg library, or a
sub-environment within a Julia REPL accessed by pressing `\texttt{]}', tracks and
maintains the dependency graph of a Julia project. State is entirely stored
within two human-readable files - \texttt{Project.toml} (which represents the direct
dependencies of the project) and \texttt{Manifest.toml} (which preserves the exact
resulting environment, including secondary dependencies and exact versions).
The user of a Julia codebase can easily reproduce the exact environment used by
the codebase as long as these files are provided.

The Julia Package ``General Registry'' indexes packages and their releases, and
(as with other languages such as Rust and Javascript) relies on a public index
hosted on GitHub (with off-GitHub backups). A help environment, accessed by
`\texttt{?}' on an empty line of the REPL, allows interactive help on any keyword
or symbol known to the REPL; it is trivial to add documentation to a function or
type by simply prepending its definition with a triple-quoted string -- this is
immediately available to the help environment.

Leveraging the fact that Julia is JAOT-compiled, and that this allows its entire
standard library to be written in idiomatic Julia, the REPL also provides a
series of powerful macro utilities for inspecting the byte- and machine-code
generated for a given expression (\verb$@code_lowered$, \verb$@code_native$) and
for locating and displaying the source code for any function in the current
namespace, including from the standard library (\verb$@less$), as well as other
introspection tools. Profiling of code in the REPL is similarly directly
supported via macros (\verb$@benchmark$...), which provide detailed performance
sampling, including execution time distributions and GC invocations. Extended
introspection and profiling tools are also available in optional packages, such
as \texttt{About.jl}, which can provide information on memory layout of
datatypes and thread safety of functions.

A well-supported VSCode extension is available for the language, which also
supports the standard LSP allowing it to support development in other editors.
This includes the usual benefits of code completion (and Unicode completion for
non-ASCII characters), linting, highlighting and so on.

Finally, Julia is one of the founding languages supported by Jupyter - being the
``Ju'' in the portmanteau - and also provides other native notebook
implementations such as Pluto.jl. 

\subsection{Key Design Features for Performance}

Type system.

Multiple dispatch.

\section{Julia for Scientific Computing}
\label{sec:juliascicomp}

General adoption:~\cite{perkel-julia-science}.
    
\subsection{GPU Programming}

Julia's JAOT compilation model makes it ideal for running on GPUs. Julia
supports GPU programming for specific backends such as
\texttt{CUDA.jl}~\cite{besard2018juliagpu} for NVIDIA,
\texttt{AMDGPU.jl}~\cite{julian_samaroo_2025_14826765} for AMD,
\texttt{Metal.jl}~\cite{Besard_Metal_jl_2022} for Mac devices with M-series
chip, and \texttt{oneAPI.jl}~\cite{Besard_oneAPI_jl_2022} for Intel devices.
They require a small amount of programming development and provide the
possibility of writing kernels with granular control.

Array based calculations are trivial to execute on the GPU. The listings in
Figure~\ref{code:arraysgpu} an array is copied to the GPU and then executes a
simple operation. 

\begin{figure}[!ht]
\centering
\begin{minted}[linenos]{julia}
using CUDA

a = CuArray([1,2,3,4])
a * 2
\end{minted}
\begin{minted}[linenos]{julia}
using AMDGPU

a = ROCArray([1,2,3,4])
a * 2
\end{minted}
\begin{minted}[linenos]{julia}
using Metal

a = MtlArray([1,2,3,4])
a * 2
\end{minted}
\begin{minted}[linenos]{julia}
using oneAPI

a = oneArray([1,2,3,4])
a * 2
\end{minted}
\caption{Array based calculations for all backends}
\label{code:arraysgpu}
\end{figure}

It is not always possible to write an algorithm and run it using high-level
array abstractions. Instead you have to write your own GPU kernel and Kernel
programming is close to the native toolkits.  Writing and maintaining specific
backend code is undesirable, from a portability point of view. Therefore Julia
also supports a generic backend,
\texttt{KernelAbstractions.jl}~\cite{BESARD201929,8471188}. This allows
developers to separate the mathematical logic of their codes from the specific
GPU backend on which it will run, resulting in high code portability. This
feature is used heavily in Julia codes from \texttt{Flux.jl}~\cite{innes:2018},
for machine learning; to
\texttt{Oceananigans.jl}~\cite{Ramadhan_Oceananigans_jl_Fast_and_2020}, a fluid
dynamics code.

\subsection{Julia HPC codes}
Beyond the single-computer practices exemplified above, large-scale applications often require high-performance computing (HPC) technologies. In this domain, Julia has established itself as a strong contender alongside well-known HPC-capable programming languages such as C, C++, and Fortran. Benchmarks evaluating intra- and inter-node communication on CPUs have demonstrated that Julia introduces negligible overhead compared to native C implementations \cite{hunold2020benchmarking}. 

A particularly noteworthy achievement is the near-zero-loss integration of MPI through \texttt{MPI.jl} \cite{byrne2021mpi}, enabling efficient parallel communication. Furthermore, when it comes to computational performance, Julia has demonstrated its ability to keep up with traditional HPC technologies. For instance, in single-CPU-node scenarios, Julia's performance matches that of vendor-optimized libraries \cite{giordano2022productivity}. Similarly, performance portability studies in multi-node CPU/GPU benchmarks confirm Julia's competitiveness across architectures \cite{teichgraber2022julia,godoy2023evaluating}.

Beyond controlled benchmarking environments, Julia has also proven its capabilities in real-world HPC applications. One prominent example is the \texttt{Celeste.jl} project \cite{RiegerIEEE:2018,RiegerAAS:2019}, where developers achieved a peak performance of \(1.54\,\mathrm{PFLOPs}\) demonstrating Julia's scalability, efficiency in large-scale computations and, consequently, membership of the petaflop club.

More recently, Julia has been advancing multi-GPU and multi-CPU applications in large-scale simulations. Notable examples include \texttt{FastIce.jl}\cite{fasticejl-github} for high-resolution flow simulations and various geocomputing applications leveraging HPC-related packages like \texttt{ParallelStencils.jl}\cite{parallelstenciljl-github} and \texttt{ImplicitGlobalGrid.jl}\cite{implicitglobalgridjl-github}. These packages enable high-performance stencil computations and efficient distributed memory parallelism, reinforcing Julia’s role as a modern HPC language.

\section{Julia in HEP}

\subsection{Challenges}

High-energy physics faces significant challenges in software efficiency and
scaling for the years to come, particularly driven by the physics programme of
the high-luminosity LHC~\cite{hsfcwp}. Data volumes will rise sharply and data
complexity will also
increase~\cite{CERN-LHCC-2022-005,Software:2815292,Valassi2021}. Therefore
software efficiency and scaling are paramount. As we have seen in
Section~\ref{sec:juliascicomp}, the Julia language can achieve very efficient
execution on HPC clusters of CPUs and GPU accelerated nodes. It has also been
shown that Julia artefacts can be effectively distributed on CVMFS, so that efficient
running on, e.g., WLCG sites, would be possible~\cite{elvis_cvmfs_caching}.

As a field, HEP is already polyglot, with C++ and Python being the principle
languages, but many others co-existing. As pointed out above, there are millions
of lines of legacy code, much in C++, but also in Fortran and Python with which
any new language must interoperate. In this respect Julia is in a very strong
position. For calling into C or Fortan, Julia offers a simple \texttt{@ccall}
macro, which is a direct, no overhead, no boilerplate interface to libraries
compiled from these languages~\cite{JuliaManualCCall}. There are many existing
examples of calls to these foreign libraries being used in Julia, e.g., at a low
level Julia itself uses the OpenBLAS library~\cite{6877458}, written in C and
modern Fortran. 

Python code can be similarly wrapped with
\texttt{PythonCall.jl}~\cite{PythonCall.jl}.

C++ code is more difficult to interface to (a `feature' of C++ itself), and the
smoothest way to achieve it is to write a small wrapper in C++ to interface to
Julia, the making use of the Julia interface package
\texttt{CxxWrap.jl}~\cite{CxxWrap.jl}. This process can be automated with the
helper program WrapIt!~\cite{wrapit-github}, which can generate wrapper code
automatically from C++ headers. With this, wrapping very large libraries becomes
much easier, as shown below in Section~\ref{sec:simulation}.

The other major desiderata for a programming language is that it is efficient
for programmers -- and for HEP this must cover the spectrum from novice coders
to experienced software engineers. As illustrated in Section
\ref{sec:juliainpractice} Julia's development ecosystem is optimised for
efficient code development~\cite{perkel-julia-science}. Features such as the
REPL, as well as notebooks, are hugely helpful at the prototyping stage.
Effective dependency tracking and package management ensure that environments
are reproducible (co-development on different platforms, such as different Linux
distributions, OS X and Windows is generally a breeze). High-level abstractions
make Julia highly expressive, leading to efficient, compact code. Finally, the
JAOT model, combined with types, makes code optimisation, from prototype to
production, generally a smoother path than in other language environments.

Of course, a language having suitable \emph{general} features, does not
automatically mean that everything needed by researchers in a particular field,
such as high-energy physics, have the tools and packages that they need. In the
next sections we shall outline the growing ecosystem of packages specifically
developed for HEP that make is possible to be productive with Julia from day 1
of coding. 

\subsection{HEP Data Formats}

We can read that data... UpROOT.jl. EDM4hep.jl.

\subsection{Event Generators}

The numerical calculation of amplitudes for hard scattering processes and Monte
Carlo event generation plays a fundamental role in high-energy physics analysis
workflows, with a long history (\cite{campbell2024event}). Consequently,
integrating various aspects of physics models with specialized numerical
algorithms in a high-performance computing environment is recognized as one
major challenge in HEP software development for future HEP experiments
(\cite{HEPSoftwareFoundation:2017ggl, HSFPhysicsEventGeneratorWG:2020gxw,
HSFPhysicsEventGeneratorWG:2021xti}) and beyond.

With the open-source framework \texttt{QuantumElectrodynamics.jl}
\cite{qedjl-github}, we take the first steps in exploring how the Julia
programming language can address these challenges. Specifically, our framework
facilitates the numerical calculation of scattering amplitudes and the
implementation of Monte Carlo event generation within the domain of perturbative
and strong-field quantum electrodynamics (cf. \cite{Fedotov:2022ely}). The
overall structure of the framework is unified through interfaces defined in
\texttt{QEDbase.jl}, which provides standardized representations for fundamental
mathematical objects such as four-momenta, bi-spinors, and phase space points.
Additionally, these interfaces extend to the configuration entities like
scattering processes, computational models, and phase space layouts. Further
interfaces are provided for computable quantities, such as differential
cross-sections, and various samplers for Monte Carlo event generation. Here,
Julia’s multiple dispatch mechanism proves particularly powerful, allowing
different implementations to be seamlessly integrated without explicitly
specifying types within the interface definitions. Moreover, the domain-specific
language facilitated by these interfaces—covering elements such as processes,
models, and phase space layouts—combined with multiple dispatch enables the
straightforward incorporation of analytical formulas whenever they are
available. This typically reduces to implementing a single method for a specific
function signature.

Beyond its interface definitions, \texttt{QuantumElectrodynamics.jl} provides
concrete implementations for all major components: \texttt{QEDcore.jl} handles
the fundamental mathematical structures, \texttt{QEDprocesses.jl} computes
differential cross sections, and \texttt{QEDevents.jl} offers different samplers
for event generation. The \texttt{QEDFeynmanDiagrams.jl} package, as part of
\texttt{QEDprocesses.jl}, facilitates the calculation of scattering amplitudes
for arbitrary QED processes by leveraging Julia's metaprogramming and code
generation capabilities. Built on top of \texttt{ComputableDAGs.jl}, the
generated code can be directly analysed and manipulated within Julia itself,
even without leaving the session. This enables meta-optimizations based on
domain-specific knowledge, such as recognizing patterns in Feynman diagrams and
exploiting mathematical properties like distributivity.

Finally, by leveraging multiple dispatch and various array abstractions from the
Julia ecosystem—such as \texttt{CUDA.jl} and \texttt{AMDGPU.jl}—the generated
code can be seamlessly compiled and executed on GPUs in addition to CPUs,
without requiring any modifications to its underlying structure. While some
framework components are still under development and will be further extended to
address broader challenges in high-energy physics, the initial structures
demonstrate great potential. The modular design, combined with Julia's
capabilities, has the potential to contribute to future developments in
numerical calculations and event generation within HEP.


\subsection{Simulation}
\label{sec:simulation}

Detector simulation is a crucial component of every High Energy Physics (HEP)
experiment, playing a key role both during the design and conception of the
detector and later in data analysis. The most widely used toolkit for this
purpose is Geant4 \cite{GEANT4:2002zbu}, a C++-based framework with over 2
million lines of code.

Given its complexity and extensive adoption, a complete rewrite of Geant4 in a
new language is highly impractical. Instead, this presents an opportunity to
explore Julia's interoperability with other languages. One particular challenge
arises from Geant4's callback-based user interface, which relies on C++ virtual
methods invoked at specific points during particle transport. Application
developers must implement these callbacks to configure and control the
simulation and extract relevant simulated data. However, integrating this
mechanism in Julia is more complex than in other languages, as Julia does not
natively support virtual methods.

The \texttt{Geant4.jl}~\cite{geant4-jl-github} package has been developed to
provide a Julia interface to Geant4~\cite{ALLISON2016186}. It leverages
\texttt{CxxWrap.jl}~\cite{CxxWrap.jl}, a package that enables calling C++
functions and types from Julia. Similar to Python's static bindings, invoking
C++ code from Julia requires explicit wrapper definitions for each method
exposed to Julia. However, given Geant4's large and complex codebase, manually
writing and maintaining these wrappers is not a viable approach, especially for
making it more sustainable with future toolkit updates. To address this, we use
WrapIt~\cite{wrapit-github}, a package that automates wrapper generation by
utilizing the Clang library to parse C++ header files and extract class
declarations. This automation significantly reduces development effort and
ensures long-term maintainability of the interface.

Integrating Geant4 with Julia allows researchers to take advantage of Julia's
high-level programming capabilities and performance benefits while retaining the
full functionality and efficiency of the Geant4 toolkit. This integration also
provided an opportunity to rethink and streamline the interface, making it more
intuitive and user-friendly. In particular, we focused on ensuring that
application developers can concentrate on the essential aspects of their
simulations while minimizing configuration overhead. Boilerplate code and C++
idiosyncrasies are hidden, allowing for a cleaner, more concise approach to
defining simulations. The performance of the \texttt{Geant4.jl} package is
comparable to that of the native C++ Geant4 toolkit, demonstrating the
feasibility of using Julia for HEP detector simulation.

\subsection{Reconstruction}

Reconstruction of HEP data is a core task for the field, and a huge amount of
software, often rather detector specific, is dedicated to that task. Even for
very detector specific code there are some early demonstrations that Julia would
be suitable, and can be competitive with modern optimised C++ on multiple
different GPU backends, e.g., for the CMS pixel detector~\cite{patatrack_julia}.

For generic reconstruction tasks there is less code in general, however, for the
task of sequential jet reconstruction, the Fastjet
package~\cite{Cacciari:2011ma} has seen very wide adoption across many
experiments. There is now a native Julia reimplementation of many of the same
algorithms as are found in Fastjet, in the \texttt{JetReconstruction.jl}
package~\cite{polyglot-jets-conference,stewart_2025_14945660}. Code ergonomics
have been found to be superior, taking advantage of many of Julia's features
such as array broadcasting and generic programming. Performance is, on average
16\% better than Fastjet for popular $pp$ reconstruction algorithms, and 33\%
better for the Durham algorithm for $e^+e^-$ events in FCCee
studies~\cite{fast-jet-reco-julia}. Using Julia's package extension mechanisms,
support for the ubiquitous EDM4hep~\cite{Gaede:2022leb} event data model is
supported directly. This adds to the user experience, where not only can the
package offer simpler, more generic interfaces than Fastjet, but there is the
ability to seamlessly hook into the rest of the Julia ecosystem, e.g., for jet
visualisation with the native Julia \texttt{Makie.jl}
package~\cite{Danisch2021}.

\subsection{Analysis}

Overview of analysis papers and suitability of Julia.

Cite some general overviews:~\cite{Stanitzki:2020bnx,eschle2023potential}.

\subsection{End-to-end Computing}

The LEGEND~\cite{LEGEND:2017AIPC} experiment demonstrates how Julia can be used
as a basis for end-to-end analysis in a larger physics experiment with multiple
subsystems. LEGEND uses Julia as its official secondary software stack, both to
verify the results of the primary software stack (written in Python) and as a
test-bed for future software technologies. The whole data analysis chain,
encompassing raw waveform data signal processing, ML-based data quality cuts,
data calibration, event building and high-level statistical analysis is
implemented completely in Julia. In addition, LEGEND uses the Bayesian Analysis
Toolkit in Julia (BAT.jl)~\cite{Schulz:2021BAT} as its primary Bayesian
framework for both background decomposition and final physics analysis, and the
Julia package SolidStateDetectors.jl~\cite{Abt:2021SSD} for detector simulation
and detector design. With the exception of a custom Geant4-based software, the
LEGEND collaboration is now able to perform any simulation and analysis task in
Julia.


\section{Conclusions}

It's all good, nothing can go wrong. To infinity and beyond, etc.

\sloppy
\raggedright
% \clearpage
\bibliography{julia-in-hep}


\end{document}
